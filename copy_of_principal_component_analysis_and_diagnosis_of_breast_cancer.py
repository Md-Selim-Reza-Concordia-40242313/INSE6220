# -*- coding: utf-8 -*-
"""Copy of Principal Component Analysis and Diagnosis of Breast Cancer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AEkJOge6kxpM3pzqWORT_2DqNd4rCZGH

# **Principal Component Analysis and diagnosis of Breast cancer**

**Install Pycaret**
"""

# !pip install pycaret==2.3.6

"""**Import Libraries**"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_theme(style="darkgrid")
import pandas as pd
plt.rcParams['figure.figsize'] = (7,5)

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

print("Pandas version: ", pd.__version__)
print("Seaborn version: ", sns.__version__)

"""**Dataset**

The “Breast Cancer diagnosis Data Set” used for Principle Component Analysis (PCA) and classification models is obtained from Kaggle. The data set provided information about tumor attributes to diagnosed it as benign or malignant for breast cancer.

There are 5 attributes with 569 entries and one column for classification.
**0 represents Benign and 1 represents Malignant.**


All attributes are numerical and they are listed bellow:

1. mean_radius = MR
2. mean_texture = MT
3. mean_perimeter = MP
4. mean_area = MA
5. mean_smoothness = MS
6. diagnosis (Label)

The original dataset can be found on below link:
https://www.kaggle.com/datasets/theodoravalerie/breast-cancer-data




"""

#read cvs file into dataframe
df = pd.read_csv('https://raw.githubusercontent.com/shreya201/INSE6220_40208796/main/Breast_cancer_data.csv')
df.head(25)

len(df.index)

df.info()

print("Number of duplicated rows is: ", df.duplicated().sum())

print("Number of rows with NaNs is: ", df.isna().any(axis=1).sum())

"""**Exploratory Data Analysis**"""

sns.pairplot(df, hue='diagnosis')
plt.show()

y =df['diagnosis']
y.value_counts().plot(kind='pie')
plt.ylabel('')
plt.show()

"""**Data Matrix**"""

X = df.drop(columns=['diagnosis'])
X.head(10)

X.describe().transpose()

"""**Standardize the Data**"""

Xs = StandardScaler().fit_transform(X)
Xcols = X.columns
X = pd.DataFrame(Xs)
X.columns = Xcols
X.head(10)

X.describe().transpose()

"""**Observations and variables**"""

observations = list(df.index)
variables = list(df.columns)

"""**Box and Whisker Plots**"""

ax = plt.figure(figsize=(15,10))
ax = sns.boxplot(data=X, orient="v", palette="Set2")
ax.set_xticklabels(ax.get_xticklabels(),rotation=45);
#ax.set_xticklabels(ax.get_xticks(), rotation=45);

# Use swarmplot() or stripplot to show the datapoints on top of the boxes:
#plt. figure()

ax = plt.figure(figsize=(15,10))
ax = sns.boxplot(data=X, orient="v", palette="Set2")
ax = sns.stripplot(data=X, color=".25")
ax.set_xticklabels(ax.get_xticklabels(),rotation=45);
#ax.set_xticklabels(ax.get_xticks(), rotation=45);

"""**Correlation Matrix**"""

fig1,ax1 = plt.subplots(figsize=(15,10))
ax = sns.heatmap(X.corr(), cmap='RdYlGn_r', linewidths=0.3, annot=True, cbar=False, square=True)
plt.yticks(rotation=0)
ax.tick_params(labelbottom=False,labeltop=True)
ax.set_xticklabels(ax.get_xticklabels(),rotation=0);
#ax.set_xticklabels(ax.get_xticks(), rotation=45);

#X.corr().style.background_gradient(cmap='coolwarm').set_precision(2)
#sns.clustermap(X.corr(), annot=True, fmt='.2f')

"""# **Principal Component Analysis (PCA)**"""

pca = PCA()
Z = pca.fit_transform(X)

# (Malignant)->1,  (Benign)->0

idx_Benign= np.where(y == 0)
idx_Malignant = np.where(y == 1)


plt. figure(figsize=(15,10))
plt.scatter(Z[idx_Benign,0], Z[idx_Benign,1], c='g', label='Benign')
plt.scatter(Z[idx_Malignant,0], Z[idx_Malignant,1], c='r', label='Malignant')


plt.legend()
plt.xlabel('$Z_1$')
plt.ylabel('$Z_2$')

"""**Eigenvectors**"""

A = pca.components_.T
plt. figure(figsize=(8,4))

plt.scatter(A[:,0],A[:,1],c='r')
plt.xlabel('$A_1$')
plt.ylabel('$A_2$')
for label, x, y in zip(variables, A[:, 0], A[:, 1]):
  plt.annotate(label, xy=(x, y), xytext=(-2, 2), textcoords='offset points', ha='right', va='bottom')
A

plt. figure(figsize=(8,4))
plt.scatter(A[:, 0],A[:, 1], marker='o', c=A[:, 2], s=A[:, 3]*500, cmap=plt.get_cmap('Spectral'))
plt.xlabel('$A_1$')
plt.ylabel('$A_2$')
for label, x, y in zip(variables,A[:, 0],A[:, 1]):
  plt.annotate(label,xy=(x, y), xytext=(-20, 20),
      textcoords='offset points', ha='right', va='bottom',
      bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),
      arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))

"""**Scree plot**"""

#Eigenvalues
Lambda = pca.explained_variance_

#Scree plot
x = np.arange(len(Lambda)) + 1
plt. figure(figsize=(15,10))
plt.plot(x,Lambda/sum(Lambda), 'ro-', lw=3)
plt.xticks(x, [""+str(i) for i in x], rotation=0)
plt.xlabel('Number of components')
plt.ylabel('Explained variance')
Lambda

"""**Explained Variance**"""

ell = pca.explained_variance_ratio_
ind = np.arange(len(ell))
plt. figure(figsize=(15,10))
plt.bar(ind, ell, align='center', alpha=0.5)
plt.plot(np.cumsum(ell))
plt.xlabel('Number of components')
plt.ylabel('Cumulative explained variance')

"""**Biplot**"""

# 0,1 denote PC1 and PC2; change values for other PCs
A1 = A[:,0]
A2 = A[:,1]
Z1 = Z[:,0]
Z2 = Z[:,1]

plt. figure(figsize=(15,10))
plt.xlabel('$Z_1$')
plt.ylabel('$Z_2$')
for i in range(len(A1)):
# arrows project features as vectors onto PC axes
  plt.arrow(0, 0, A1[i]*max(Z1), A2[i]*max(Z2), color='k', width=0.0005, head_width=0.0025)
  plt.text(A1[i]*max(Z1)*1.2, A2[i]*max(Z2)*1.2,variables[i], color='k')

plt.scatter(Z[idx_Malignant,0], Z[idx_Malignant,1], c='r', label='Malignant')
plt.scatter(Z[idx_Benign,0], Z[idx_Benign,1], c='g', label='Benign')

plt.legend(loc='upper left')
Z1
Z2

"""# **Using PCA Library**"""

# !pip install pca

from pca import pca
# Initialize and keep all PCs
model = pca()
# Fit transform
out = model.fit_transform(X)

"""**Principal Components**"""

out['PC']

"""**Scatter plot**"""

model.scatter(label=True, legend=False)

"""**Eigenvectors**"""

A = out['loadings'].T
A

sns.scatterplot(data=A, x="PC1", y="PC2")
plt.xlabel('$A_1$')
plt.ylabel('$A_2$')
for i in range(A.shape[0]):
 plt.text(x=A.PC1[i]+0.02,y=A.PC2[i]+0.02, s=variables[i],
          fontdict=dict(color='red',size=10),
          bbox=dict(facecolor='yellow',alpha=0.5))

"""**Scree Plot**"""

VR = out['variance_ratio']
x = np.arange(len(VR)) + 1
plt.plot(x, VR, 'ro-', lw=3)
plt.xticks(x, [""+str(i) for i in x], rotation=0)
plt.xlabel('Number of components')
plt.ylabel('Explained variance')
plt.show()

"""**Explained Variance Plot**"""

model.plot();

"""**Biplot**"""

model.biplot(label=False, legend=False, color_arrow='k')

model.biplot3d(legend=False)

"""# Classification using Pycaret
At first dataset is split into train and test set with 70% and 30% ratio respectively. Then using the PyCaret compare_models() function, the models which shows the highest accuracy on the original dataset can be found.
Before applying PCA, the output from compare_models() function shows that Linear Discriminant Analayis, Extra Trees Classifier, and Gradient Boosting Classifier shows the best performance.

 However, after applying PCA, the these models performance decreases and Logistic Regression, K-nearest neighbour, and Quadratic Discriminant Analysis shows the highest performance. Therefore, I have taken Logistic Regression, K-nearest Neighbour and Quadratic discriminant Analysis as my classification algorithms and applied these three models on the original dataset before applying PCA and after applying PCA.
"""

#Run the below code in your notebook to check the installed version
from pycaret.utils import version
version()

#For Google Colab only
from pycaret.utils import enable_colab
enable_colab()

data = df.sample(frac=0.9, random_state=786)
data_unseen = df.drop(data.index)
#dr = df.drop(columns=['id'])

data.reset_index(drop=True, inplace=True)
data_unseen.reset_index(drop=True, inplace=True)

print('Data for Modeling: ' + str(data.shape))
print('Unseen Data For Predictions: ' + str(data_unseen.shape))

from pycaret.classification import *
clf = setup(data=data, target='diagnosis', train_size=0.7, session_id=123)

"""**Comparing All Models**"""

#show the best model and their statistics
best_model = compare_models()

best_model

"""Create a Model

**Logistic Regresssion**
"""

lr = create_model('lr')

"""**Tune the model**"""

tuned_lr=tune_model(lr)

tuned_lr

"""**Evaluate Logistic Regression**

"""

evaluate_model(tuned_lr)

"""**Create model with K-nearest neighbour**"""

knn = create_model('knn')

"""**Tune model with K-nearest neighbour**"""

tuned_knn = tune_model(knn)

tuned_knn

"""**Evaluate model with K-nearest neighbour**"""

evaluate_model(tuned_knn)

"""**Create model with Quadratic Discriminant Analysis**"""

qda=create_model('qda')

"""**Tune QDA Model**"""

tuned_qda=tune_model(qda)

"""**Evaluate QDA Model**"""

tuned_qda

evaluate_model(tuned_qda)

# !pip install statsmodels --upgrade

"""# **Classification + PCA**"""

clf_pca = setup(data=data, target='diagnosis', train_size=0.7, session_id=123, normalize = True, pca = True, pca_components = 3)

"""# **Comparing Models**"""

#show the best model and their statistics
best_model_pca = compare_models()

"""# **Best model with PCA**"""

best_model_pca

"""# **Tune Best Model**"""

# Tune hyperparameters with scikit-learn (default)
tuned_best_model_pca = tune_model(best_model_pca)

"""# **Evaluate Best Model**"""

evaluate_model(tuned_best_model_pca)

lr_pca = create_model('lr')

"""# **Tune Model**"""

tuned_lr_pca = tune_model(lr_pca)

"""# **Evaluate Model**"""

tuned_lr_pca

evaluate_model(tuned_lr_pca)

"""# **Create K-nearest neighbour Model**"""

knn_pca = create_model('knn')

"""# **Tune Model**"""

tuned_knn_pca = tune_model(knn_pca)

tuned_knn_pca

"""# **Evaluate Model KNN**"""

evaluate_model(tuned_knn_pca)

"""# **Create Quadratic Discriminant Analysis Model**"""

qda_pca = create_model('qda')

"""# **Tune model**"""

tuned_qda_pca = tune_model(qda_pca)

"""# **Evaluate Model**"""

tuned_qda_pca

evaluate_model(tuned_qda_pca)

"""# Explainable AI with Shapley values
For binary classification, AI shapley can only support tree based classification models. As none of the the best three models with highest accuracy (LR, KNN, QDA) based on tree, I chose the fourth best model "Extra trees classifier" for the AI Shapley analysis.
"""

et_pca = create_model('et')

tuned_et_pca = tune_model(et_pca)

tuned_et_pca

evaluate_model(tuned_et_pca)

# !pip install shap

import shap

interpret_model(tuned_et_pca, plot='summary')

interpret_model(tuned_et_pca, plot='reason', observation=32)

interpret_model(tuned_et_pca, plot='reason')